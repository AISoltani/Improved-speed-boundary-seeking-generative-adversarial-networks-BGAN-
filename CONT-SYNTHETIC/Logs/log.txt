Testing DCGAN + modified Loss...
Loading data...
Num_epochs: 100, initial_eta: 0.0002, lambda1: 1e-05, lambda2: 0.001, lambda3: 1000.0
Building model and compiling GAN functions...
Starting training of GAN...
Total Epoch 1 of 100 took 0.035s
  training loss:	[ 0.60967742  0.36451613  0.74181265 -0.73784953  1.48122269  0.74752292]
Total Epoch 2 of 100 took 0.035s
  training loss:	[ 0.60967742  0.36032258  0.74131322 -0.73780525  1.47931692  0.74611564]
Total Epoch 3 of 100 took 0.034s
  training loss:	[ 0.61        0.34935484  0.74081266 -0.73777854  1.47852611  0.74582462]
Total Epoch 4 of 100 took 0.034s
  training loss:	[ 0.61        0.35419355  0.7403096  -0.73777783  1.47664249  0.74444356]
Total Epoch 5 of 100 took 0.034s
  training loss:	[ 0.61        0.35967742  0.7397998  -0.7378177   1.47545512  0.74376588]
Total Epoch 6 of 100 took 0.035s
  training loss:	[ 0.61        0.3516129   0.73927593 -0.73789841  1.47407095  0.74290585]
Total Epoch 7 of 100 took 0.035s
  training loss:	[ 0.61032258  0.34709677  0.73873818 -0.73800558  1.47263782  0.74201101]
Total Epoch 8 of 100 took 0.035s
  training loss:	[ 0.61032258  0.34709677  0.73819464 -0.73815626  1.4710891   0.74100678]
Total Epoch 9 of 100 took 0.035s
  training loss:	[ 0.61096774  0.34483871  0.73764819 -0.73832738  1.46927785  0.73974315]
Total Epoch 10 of 100 took 0.035s
  training loss:	[ 0.61096774  0.34        0.73710042 -0.73848468  1.46766671  0.73868081]
Total Epoch 11 of 100 took 0.035s
  training loss:	[ 0.61129032  0.33677419  0.73655045 -0.73862004  1.46579008  0.73735495]
Total Epoch 12 of 100 took 0.035s
  training loss:	[ 0.61096774  0.34032258  0.73600018 -0.73873836  1.46399509  0.73611087]
Total Epoch 13 of 100 took 0.034s
  training loss:	[ 0.61193548  0.34258065  0.7354511  -0.73882979  1.4625682   0.73523342]
Total Epoch 14 of 100 took 0.034s
  training loss:	[ 0.61193548  0.33096774  0.73490471 -0.73890901  1.46079625  0.73400811]
Total Epoch 15 of 100 took 0.035s
  training loss:	[ 0.61225806  0.32967742  0.73436075 -0.73896879  1.45875193  0.7325078 ]
Total Epoch 16 of 100 took 0.033s
  training loss:	[ 0.61258065  0.33064516  0.73381954 -0.73900682  1.45982431  0.73412123]
Total Epoch 17 of 100 took 0.032s
  training loss:	[ 0.61290323  0.3316129   0.7332806  -0.73903006  1.45667302  0.73150858]
Total Epoch 18 of 100 took 0.032s
  training loss:	[ 0.61322581  0.3316129   0.73274428 -0.73903728  1.45612858  0.73149999]
Total Epoch 19 of 100 took 0.032s
  training loss:	[ 0.61354839  0.32483871  0.73221058 -0.73902673  1.4546397   0.73054418]
Total Epoch 20 of 100 took 0.032s
  training loss:	[ 0.61387097  0.32        0.73167717 -0.73899841  1.4530396   0.72947666]
Total Epoch 21 of 100 took 0.033s
  training loss:	[ 0.61387097  0.33        0.73114282 -0.73896211  1.45243778  0.7294083 ]
Total Epoch 22 of 100 took 0.032s
  training loss:	[ 0.61419355  0.32612903  0.73060578 -0.73891962  1.45153615  0.72904274]
Total Epoch 23 of 100 took 0.032s
  training loss:	[ 0.61419355  0.32774194  0.73006439 -0.7388702   1.44947968  0.72752663]
Total Epoch 24 of 100 took 0.032s
  training loss:	[ 0.61451613  0.33516129  0.72951567 -0.73880684  1.44821753  0.72681202]
Total Epoch 25 of 100 took 0.032s
  training loss:	[ 0.61483871  0.33419355  0.72895652 -0.73872793  1.44732062  0.72647291]
Total Epoch 26 of 100 took 0.033s
  training loss:	[ 0.61548387  0.32870968  0.72838455 -0.7386328   1.4466025   0.72632523]
Total Epoch 27 of 100 took 0.032s
  training loss:	[ 0.61548387  0.33096774  0.72779882 -0.73850721  1.44592915  0.72623577]
Total Epoch 28 of 100 took 0.032s
  training loss:	[ 0.61548387  0.32645161  0.72720104 -0.7383762   1.44526926  0.72617176]
Total Epoch 29 of 100 took 0.032s
  training loss:	[ 0.61612903  0.33419355  0.72659492 -0.73825967  1.44463212  0.72613896]
Total Epoch 30 of 100 took 0.032s
  training loss:	[ 0.61612903  0.33032258  0.725995   -0.7381891   1.44306141  0.72516687]
Total Epoch 31 of 100 took 0.032s
  training loss:	[ 0.61709677  0.32903226  0.72540671 -0.73814708  1.44250673  0.72519948]
Total Epoch 32 of 100 took 0.031s
  training loss:	[ 0.61870968  0.32741935  0.7248354  -0.73817152  1.44139909  0.72466282]
Total Epoch 33 of 100 took 0.031s
  training loss:	[ 0.61935484  0.32903226  0.72428876 -0.7382099   1.43999288  0.72380308]
Total Epoch 34 of 100 took 0.031s
  training loss:	[ 0.61935484  0.32096774  0.72376668 -0.73829186  1.43937974  0.72371231]
Total Epoch 35 of 100 took 0.031s
  training loss:	[ 0.62064516  0.32451613  0.72325933 -0.7383219   1.43895517  0.72379489]
Total Epoch 36 of 100 took 0.032s
  training loss:	[ 0.62064516  0.32677419  0.7227698  -0.73829138  1.43818085  0.72350932]
Total Epoch 37 of 100 took 0.031s
  training loss:	[ 0.62129032  0.32483871  0.72229815 -0.73829967  1.43724349  0.72304321]
Total Epoch 38 of 100 took 0.031s
  training loss:	[ 0.6216129   0.31741935  0.72183233 -0.73836601  1.43641319  0.72267892]
Total Epoch 39 of 100 took 0.031s
  training loss:	[ 0.62193548  0.32967742  0.72136295 -0.73836923  1.43545912  0.72219381]
Total Epoch 40 of 100 took 0.031s
  training loss:	[ 0.62193548  0.32451613  0.72088945 -0.73842019  1.43482162  0.72202984]
Total Epoch 41 of 100 took 0.032s
  training loss:	[ 0.62193548  0.3283871   0.72042704 -0.73847288  1.43422117  0.72189187]
Total Epoch 42 of 100 took 0.031s
  training loss:	[ 0.62193548  0.32645161  0.71998817 -0.73851293  1.43412588  0.72223541]
Total Epoch 43 of 100 took 0.031s
  training loss:	[ 0.62193548  0.33064516  0.71956837 -0.73855072  1.43252983  0.72105911]
Total Epoch 44 of 100 took 0.031s
  training loss:	[ 0.62225806  0.3216129   0.71916395 -0.73861015  1.433131    0.72206489]
Total Epoch 45 of 100 took 0.031s
  training loss:	[ 0.62322581  0.31935484  0.71877187 -0.73868912  1.43094302  0.72026938]
Total Epoch 46 of 100 took 0.032s
  training loss:	[ 0.62387097  0.32612903  0.71838951 -0.73878181  1.43167273  0.721382  ]
Total Epoch 47 of 100 took 0.031s
  training loss:	[ 0.62387097  0.32870968  0.71801311 -0.73887932  1.43093814  0.72102441]
Total Epoch 48 of 100 took 0.031s
  training loss:	[ 0.62387097  0.32290323  0.71764082 -0.73895687  1.43067796  0.72113693]
Total Epoch 49 of 100 took 0.031s
  training loss:	[ 0.62419355  0.31741935  0.71727037 -0.73902845  1.42951233  0.72034209]
Total Epoch 50 of 100 took 0.031s
  training loss:	[ 0.62483871  0.32032258  0.71690029 -0.73910975  1.42898503  0.72018532]
Total Epoch 51 of 100 took 0.032s
  training loss:	[ 0.62483871  0.32580645  0.71652961 -0.7392019   1.42894411  0.72051563]
Total Epoch 52 of 100 took 0.031s
  training loss:	[ 0.62483871  0.31612903  0.71615785 -0.73930568  1.42697147  0.71891541]
Total Epoch 53 of 100 took 0.031s
  training loss:	[ 0.62580645  0.33645161  0.71578431 -0.73943329  1.42761488  0.71993326]
Total Epoch 54 of 100 took 0.031s
  training loss:	[ 0.62612903  0.3383871   0.71540827 -0.73959124  1.42653696  0.71923259]
Total Epoch 55 of 100 took 0.031s
  training loss:	[ 0.62677419  0.33806452  0.71502948 -0.73975128  1.42606401  0.71913965]
Total Epoch 56 of 100 took 0.032s
  training loss:	[ 0.62741935  0.32225806  0.71464694 -0.7399165   1.42502226  0.71848171]
Total Epoch 57 of 100 took 0.031s
  training loss:	[ 0.62806452  0.33451613  0.71426159 -0.74009341  1.42447814  0.71832432]
Total Epoch 58 of 100 took 0.031s
  training loss:	[ 0.6283871   0.33903226  0.71387386 -0.74029362  1.42474764  0.71898316]
Total Epoch 59 of 100 took 0.031s
  training loss:	[ 0.62903226  0.34548387  0.71348429 -0.74050915  1.42404704  0.7186739 ]
Total Epoch 60 of 100 took 0.031s
  training loss:	[ 0.62903226  0.34419355  0.71309143 -0.74073255  1.42241187  0.71743343]
Total Epoch 61 of 100 took 0.032s
  training loss:	[ 0.62903226  0.34387097  0.7126956  -0.74096662  1.42199242  0.71741176]
Total Epoch 62 of 100 took 0.031s
  training loss:	[ 0.62935484  0.36129032  0.71229792 -0.74124199  1.4219253   0.71774467]
Total Epoch 63 of 100 took 0.031s
  training loss:	[ 0.63        0.37193548  0.71190017 -0.74158895  1.4194176   0.71563779]
Total Epoch 64 of 100 took 0.031s
  training loss:	[ 0.63        0.38516129  0.71150446 -0.74202001  1.41980381  0.71642363]
Total Epoch 65 of 100 took 0.031s
  training loss:	[ 0.63        0.40258065  0.71111047 -0.7424885   1.41943649  0.71645453]
Total Epoch 66 of 100 took 0.032s
  training loss:	[ 0.63        0.42032258  0.71071881 -0.74296129  1.41833379  0.71574783]
Total Epoch 67 of 100 took 0.031s
  training loss:	[ 0.63064516  0.42935484  0.71032977 -0.74342996  1.41804169  0.71584906]
Total Epoch 68 of 100 took 0.031s
  training loss:	[ 0.63129032  0.4483871   0.70994276 -0.74388945  1.41777054  0.71596914]
Total Epoch 69 of 100 took 0.031s
  training loss:	[ 0.63193548  0.48741935  0.70955676 -0.74433655  1.41604612  0.7146348 ]
Total Epoch 70 of 100 took 0.031s
  training loss:	[ 0.63290323  0.51645161  0.70917124 -0.74478656  1.4161473   0.71512562]
Total Epoch 71 of 100 took 0.032s
  training loss:	[ 0.63387097  0.53290323  0.70878655 -0.74524635  1.41539872  0.71476593]
Total Epoch 72 of 100 took 0.031s
  training loss:	[ 0.63451613  0.56290323  0.70840216 -0.74571306  1.41363059  0.71338648]
Total Epoch 73 of 100 took 0.031s
  training loss:	[ 0.63516129  0.58096774  0.70801836 -0.74616826  1.41284623  0.71299008]
Total Epoch 74 of 100 took 0.031s
  training loss:	[ 0.63580645  0.62774194  0.70763499 -0.74664009  1.41255952  0.71309108]
Total Epoch 75 of 100 took 0.031s
  training loss:	[ 0.63709677  0.61483871  0.70725179 -0.74712735  1.41082915  0.7117484 ]
Total Epoch 76 of 100 took 0.032s
  training loss:	[ 0.63806452  0.64967742  0.70686775 -0.74763024  1.41012263  0.71143057]
Total Epoch 77 of 100 took 0.031s
  training loss:	[ 0.6383871   0.65903226  0.70648324 -0.74815285  1.41013648  0.7118337 ]
Total Epoch 78 of 100 took 0.031s
  training loss:	[ 0.63935484  0.66387097  0.70609814 -0.74870986  1.40868919  0.71077671]
Total Epoch 79 of 100 took 0.031s
  training loss:	[ 0.64        0.68        0.7057116  -0.74928743  1.40794921  0.71042865]
Total Epoch 80 of 100 took 0.031s
  training loss:	[ 0.64032258  0.68        0.70532513 -0.74984235  1.40680038  0.70967146]
Total Epoch 81 of 100 took 0.032s
  training loss:	[ 0.64032258  0.68387097  0.70493907 -0.75036764  1.40622167  0.70948368]
Total Epoch 82 of 100 took 0.031s
  training loss:	[ 0.64064516  0.67709677  0.70455313 -0.75089151  1.40566366  0.70931646]
Total Epoch 83 of 100 took 0.031s
  training loss:	[ 0.64129032  0.67645161  0.70416743 -0.75141746  1.40495787  0.70900124]
Total Epoch 84 of 100 took 0.031s
  training loss:	[ 0.64129032  0.66870968  0.70378172 -0.75190187  1.40420487  0.7086384 ]
Total Epoch 85 of 100 took 0.031s
  training loss:	[ 0.64096774  0.6683871   0.70339578 -0.7523495   1.40286388  0.70768744]
Total Epoch 86 of 100 took 0.035s
  training loss:	[ 0.64258065  0.66967742  0.70300996 -0.75276268  1.40200406  0.70721719]
Total Epoch 87 of 100 took 0.035s
  training loss:	[ 0.64258065  0.66322581  0.70262498 -0.75313383  1.4010937   0.70669514]
Total Epoch 88 of 100 took 0.035s
  training loss:	[ 0.64322581  0.66        0.70223933 -0.75346649  1.40069925  0.70668928]
Total Epoch 89 of 100 took 0.035s
  training loss:	[ 0.64322581  0.66290323  0.70185423 -0.75376391  1.40006119  0.70643891]
Total Epoch 90 of 100 took 0.035s
  training loss:	[ 0.64387097  0.64516129  0.70146865 -0.75402516  1.40011669  0.70688216]
Total Epoch 91 of 100 took 0.035s
  training loss:	[ 0.64419355  0.63935484  0.70108283 -0.75424188  1.39869242  0.70584549]
Total Epoch 92 of 100 took 0.035s
  training loss:	[ 0.64451613  0.64        0.70069677 -0.75441486  1.39818428  0.70572476]
Total Epoch 93 of 100 took 0.035s
  training loss:	[ 0.64451613  0.63903226  0.70030987 -0.75455505  1.39818366  0.70611204]
Total Epoch 94 of 100 took 0.035s
  training loss:	[ 0.64419355  0.62419355  0.69992143 -0.75465113  1.39690728  0.70522468]
Total Epoch 95 of 100 took 0.035s
  training loss:	[ 0.64451613  0.62451613  0.69953054 -0.75471443  1.39607206  0.70478059]
Total Epoch 96 of 100 took 0.035s
  training loss:	[ 0.64419355  0.62548387  0.69913733 -0.75468695  1.39705116  0.70615224]
Total Epoch 97 of 100 took 0.035s
  training loss:	[ 0.64451613  0.61258065  0.69874221 -0.75461316  1.39645222  0.70594728]
Total Epoch 98 of 100 took 0.035s
  training loss:	[ 0.64451613  0.61290323  0.69834423 -0.75449741  1.39457207  0.70446356]
Total Epoch 99 of 100 took 0.035s
  training loss:	[ 0.64451613  0.59225806  0.69794309 -0.75430113  1.39506798  0.70535824]
Total Epoch 100 of 100 took 0.035s
  training loss:	[ 0.64483871  0.60645161  0.69753873 -0.75402439  1.39500828  0.70569973]
